import torch
import os
import numpy as np
import io
import cv2
from collections import defaultdict, deque
from ts.torch_handler.base_handler import BaseHandler
from ultralytics import YOLO
import torch.nn as nn
from transformer_model import TransformerPoseClassifier  # âœ… Transformer ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°


class PoseEstimatorHandler(BaseHandler):
    def __init__(self):
        """í•¸ë“¤ëŸ¬ ì´ˆê¸°í™”"""
        super().__init__()
        self.device = None
        self.pose_model = None
        self.transformer_model = None
        self.class_names = ["running", "walking", "sitting", "lying"]
        self.sequence_buffers = None
        self.initialized = False

    def initialize(self, context):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # ëª¨ë¸ ë””ë ‰í† ë¦¬ ê²½ë¡œ í™•ì¸
        properties = context.system_properties
        model_dir = properties.get("model_dir")
        if not model_dir:
            raise ValueError("âŒ Model directory not specified in context")

        # ëª¨ë¸ íŒŒì¼ ê²½ë¡œ í™•ì¸
        yolo_path = os.path.join(model_dir, "yolov8n-pose.pt")
        transformer_path = os.path.join(model_dir, "pose_estimator.pth")

        if not os.path.exists(yolo_path):
            raise FileNotFoundError(f"âŒ YOLO model not found at {yolo_path}")
        if not os.path.exists(transformer_path):
            raise FileNotFoundError(f"âŒ Transformer model not found at {transformer_path}")

        # âœ… YOLO ëª¨ë¸ ë¡œë“œ
        try:
            self.pose_model = YOLO(yolo_path).to(self.device)
            self.pose_model.eval()
        except Exception as e:
            raise RuntimeError(f"âŒ Failed to load YOLO model: {str(e)}")

        # âœ… Transformer ëª¨ë¸ ë¡œë“œ
        try:
            checkpoint = torch.load(transformer_path, map_location=self.device)
            self.transformer_model = TransformerPoseClassifier(
                input_dim=34,  # 17ê°œ ëœë“œë§ˆí¬ * 2 (x, y)
                output_dim=len(self.class_names),
                num_heads=4,
                num_layers=4,
                hidden_dim=128,
                dropout=0.5,
                max_len=96
            ).to(self.device)

            self.transformer_model.load_state_dict(checkpoint, strict=False)
            self.transformer_model.eval()
        except Exception as e:
            raise RuntimeError(f"âŒ Failed to load Transformer model: {str(e)}")

        # âœ… ì‹œí€€ìŠ¤ ë²„í¼ ì´ˆê¸°í™” (ìµœëŒ€ 5ëª… ì¶”ì )
        self.sequence_buffers = defaultdict(lambda: deque(maxlen=96))
        self.initialized = True

        print("âœ… Pose Estimator Models loaded successfully")
        print(f"ğŸ“Œ Model directory: {model_dir}")
        print(f"ğŸ“Œ Device: {self.device}")

    def preprocess(self, data):
        """ ì˜ìƒì„ í”„ë ˆì„ ë‹¨ìœ„ë¡œ ë³€í™˜ (MP4 â†’ Frame) """
        try:
            video_bytes = data[0].get("body", None)
            if video_bytes is None:
                raise ValueError("âŒ Received empty video data")

            # MP4 íŒŒì¼ ì €ì¥ í›„ OpenCVë¡œ ë¡œë“œ
            video_path = "/tmp/temp_video.mp4"
            with open(video_path, "wb") as f:
                f.write(video_bytes)

            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise ValueError("âŒ VideoCapture failed to open video file")

            frames = []
            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                # âœ… YOLOv8 Pose ì…ë ¥ í¬ê¸°ë¡œ ì¡°ì •
                target_size = (320, 256)
                frame = cv2.resize(frame, target_size)
                frames.append(frame)

            cap.release()

            if len(frames) == 0:
                raise ValueError("âŒ No valid frames found in video")

            print(f"ğŸ“Œ Total Frames Extracted: {len(frames)}")
            return {"frames": frames}

        except Exception as e:
            print(f"ğŸ”¥ Preprocessing error: {e}")
            return None

    def extract_landmarks(self, frame):
        """ YOLOv8 Poseë¥¼ ì‚¬ìš©í•´ ëœë“œë§ˆí¬ ì¶”ì¶œ """
        results = self.pose_model(frame)
        height, width, _ = frame.shape
        people_data = []

        if len(results) > 0 and hasattr(results[0], 'keypoints'):
            keypoints = results[0].keypoints.xy.cpu().numpy()  # (N, 17, 2)
            for person_id, landmarks in enumerate(keypoints):
                keypoints_normalized = landmarks / [width, height]  # ì •ê·œí™”
                people_data.append((person_id, keypoints_normalized))
        return people_data

    def predict_class(self, sequence):
        """ Transformer ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë™ì‘(class) ì˜ˆì¸¡ """
        if len(sequence) < 96:
            return "Waiting for data..."

        input_tensor = torch.tensor(np.array(sequence), dtype=torch.float32).unsqueeze(0).to(self.device)

        with torch.no_grad():
            output = self.transformer_model(input_tensor)
            _, predicted = torch.max(output, 1)
        return self.class_names[predicted.item()]

    def inference(self, data):
        """ YOLO Pose â†’ Transformer ë™ì‘ ì¸ì‹ ì‹¤í–‰ """
        frames = data["frames"]
        results = []

        for frame in frames:
            # âœ… YOLO Pose ì‹¤í–‰ (ë°”ìš´ë”© ë°•ìŠ¤ X, ëœë“œë§ˆí¬ë§Œ)
            people_data = self.extract_landmarks(frame)

            if not people_data:
                results.append({"frame_id": len(results), "pose": "No person detected"})
                continue

            frame_results = []
            for person_id, landmarks in people_data:
                self.sequence_buffers[person_id].append(landmarks.flatten())  # (34,)

                predicted_class = self.predict_class(self.sequence_buffers[person_id])
                frame_results.append({"person_id": person_id, "pose": predicted_class})

            results.append({"frame_id": len(results), "poses": frame_results})

        return results

    def postprocess(self, inference_output):
        """ TorchServe ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ """
        return inference_output
