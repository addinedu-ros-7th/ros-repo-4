import torch
import os
import io
import cv2
import json
import numpy as np
from PIL import Image
from ts.torch_handler.base_handler import BaseHandler
from identity_model import ImprovedGenderAgeModel, AgeNormalizer
import torchvision.transforms as transforms

class IdentityEstimatorHandler(BaseHandler):
    def initialize(self, context):
        """ ëª¨ë¸ ì´ˆê¸°í™” """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model_dir = context.system_properties.get("model_dir")
        model_path = os.path.join(model_dir, "identity_estimator.pth")  

        # ëª¨ë¸ ë¡œë“œ
        self.model = ImprovedGenderAgeModel(pretrained=False).to(self.device)
        self.model.load_state_dict(torch.load(model_path, map_location=self.device))
        self.model.eval()

        # ë°ì´í„° ì „ì²˜ë¦¬ ì •ì˜
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),  
            transforms.ToTensor(),         
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])

        # ë‚˜ì´ ì •ê·œí™” ê°ì²´
        self.age_normalizer = AgeNormalizer()

        print("âœ… Identity Estimator Model loaded successfully")

    def preprocess(self, data):
        """ ì˜ìƒ ì „ì²˜ë¦¬ (ë¹„ë””ì˜¤ â†’ ì—¬ëŸ¬ í”„ë ˆì„) """
        try:
            video_bytes = data[0].get("body", None)
            if video_bytes is None:
                raise ValueError("âŒ Received empty video data")

            # ì„ì‹œ íŒŒì¼ ì €ì¥ í›„ OpenCVë¡œ ë¡œë“œ
            video_path = "/tmp/temp_video.mp4"
            with open(video_path, "wb") as f:
                f.write(video_bytes)

            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise ValueError("âŒ VideoCapture failed to open video file")

            frames = []
            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                # OpenCV BGR â†’ RGB ë³€í™˜ í›„ PIL ì´ë¯¸ì§€ ë³€í™˜
                rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                pil_image = Image.fromarray(rgb_frame)

                # ğŸš€ ì „ì²˜ë¦¬ í›„ Tensor ë³€í™˜
                input_tensor = self.transform(pil_image).unsqueeze(0).to(self.device)
                frames.append(input_tensor)

            cap.release()

            if len(frames) == 0:
                raise ValueError("âŒ No valid frames found in video")

            print(f"ğŸ“Œ Total Frames Extracted: {len(frames)}")
            return {"frames": frames}

        except Exception as e:
            print(f"ğŸ”¥ Preprocessing error: {e}")
            return None

    def inference(self, data):
        """ ëª¨ë¸ ì¶”ë¡  (ì—¬ëŸ¬ í”„ë ˆì„ ì§€ì›) """
        predictions = []
        with torch.no_grad():
            for frame in data["frames"]:
                age_output, gender_output = self.model(frame)
                gender_prob = torch.sigmoid(gender_output).item()
                age = self.age_normalizer.denormalize(age_output.item())

                gender = "Female" if gender_prob > 0.5 else "Male"
                confidence = max(gender_prob, 1 - gender_prob) * 100

                predictions.append({"age": round(age, 2), "gender": gender, "confidence": round(confidence, 2)})

        return predictions

    def postprocess(self, data):
        """ í›„ì²˜ë¦¬: JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ """
        try:
            return [{"age": d["age"], "gender": d["gender"], "confidence": d["confidence"]} for d in data]  # âœ… ì˜¬ë°”ë¥¸ í˜•ì‹
        except Exception as e:
            print(f"ğŸ”¥ Postprocessing error: {e}")
            return [{"error": str(e)}]
