import torch
import os
import numpy as np
import io
import cv2
from PIL import Image
from ts.torch_handler.base_handler import BaseHandler
from ultralytics import YOLO
from concurrent.futures import ThreadPoolExecutor

class FireDetectionHandler(BaseHandler):
    def initialize(self, context):
        """ ğŸ”¥ ëª¨ë¸ ì´ˆê¸°í™” """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model_dir = context.system_properties.get("model_dir")
        model_path = os.path.join(model_dir, "fire.pt")

        # YOLOv8 ëª¨ë¸ ë¡œë“œ
        self.model = YOLO(model_path).to(self.device)
        self.model.eval()

        self.confidence_threshold = float(os.environ.get("CONFIDENCE_THRESHOLD", 0.2))

        print(f"âœ… Fire Detection Model (YOLOv8) Loaded Successfully with threshold {self.confidence_threshold}")

    def preprocess_single_video(self, video_idx, video_bytes):
        """ ğŸ”¥ ê°œë³„ ì˜ìƒ ì „ì²˜ë¦¬ """
        try:
            video_path = f"/tmp/fire_video_{video_idx}.mp4"
            with open(video_path, "wb") as f:
                f.write(video_bytes)

            cap = cv2.VideoCapture(video_path)
            if not cap.isOpened():
                raise ValueError(f"âŒ VideoCapture failed to open video file {video_idx}")

            frames = []
            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                # ğŸš€ YOLOv8 ì…ë ¥ ì‚¬ì´ì¦ˆë¡œ ë¦¬ì‚¬ì´ì§• (32ì˜ ë°°ìˆ˜)
                target_size = (320, 256)  # YOLO ëª¨ë¸ì˜ ìµœì  ì…ë ¥ í¬ê¸°
                frame = cv2.resize(frame, target_size)
                frame = torch.from_numpy(frame).permute(2, 0, 1).float().to(self.device) / 255.0
                frames.append(frame)

            cap.release()

            if len(frames) == 0:
                raise ValueError(f"âŒ No valid frames found in video {video_idx}")

            print(f"ğŸ“Œ Video {video_idx} - Total Frames Extracted: {len(frames)}")
            return {"frames": frames}

        except Exception as e:
            print(f"ğŸ”¥ Preprocessing error in Video {video_idx}: {e}")
            return None

    def preprocess(self, data, num_workers=2):
        """ ğŸ”¥ ë‹¤ì¤‘ ì˜ìƒ ë³‘ë ¬ ì „ì²˜ë¦¬ """
        videos = []
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [executor.submit(self.preprocess_single_video, idx, video_data.get("body", None))
                       for idx, video_data in enumerate(data)]
            
            for future in futures:
                result = future.result()
                if result:
                    videos.append(result)
                else:
                    videos.append({"frames": []})  # ì‹¤íŒ¨í•œ ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜

        return {"videos": videos}

    def inference_single_video(self, video_idx, frames):
        """ ğŸ”¥ ê°œë³„ ì˜ìƒ ì¶”ë¡  """
        if len(frames) == 0:
            return []

        with torch.no_grad():
            results = [self.model(frame.unsqueeze(0), verbose=False)[0] for frame in frames]
        return results

    def inference(self, data, num_workers=2):
        """ ğŸ”¥ ë‹¤ì¤‘ ì˜ìƒ ë³‘ë ¬ ì¶”ë¡  """
        results = []
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [executor.submit(self.inference_single_video, idx, video_data["frames"])
                       for idx, video_data in enumerate(data["videos"])]

            for future in futures:
                results.append(future.result())

        return results

    def postprocess_single_video(self, video_idx, detections):
        """ ğŸ”¥ ê°œë³„ ì˜ìƒ í›„ì²˜ë¦¬ """
        predictions = []
        
        for result in detections:
            boxes = result.boxes.xyxy.cpu().numpy() if hasattr(result.boxes, 'xyxy') else []
            scores = result.boxes.conf.cpu().numpy() if hasattr(result.boxes, 'conf') else []
            classes = result.boxes.cls.cpu().numpy() if hasattr(result.boxes, 'cls') else []

            frame_predictions = []
            for i in range(len(boxes)):
                x_min, y_min, x_max, y_max = boxes[i]
                
                confidence = float(scores[i])

                # âœ… Confidence Threshold ì ìš© (ê¸°ë³¸ê°’: 0.5)
                if confidence < self.confidence_threshold:
                    continue

                class_mapping = {0: 100, 1: 101}  # 0: ë¶ˆ â†’ 100, 1: ì—°ê¸° â†’ 101
                adjusted_class_id = class_mapping.get(int(classes[i]), 199)

                frame_predictions.append({
                    "bbox": [float(x_min), float(y_min), float(x_max), float(y_max)],
                    "confidence": float(scores[i]),
                    "class_id": adjusted_class_id
                })

            # ğŸ”¥ íƒì§€ëœ ê°ì²´ê°€ ìˆëŠ” ê²½ìš°ë§Œ predictionsì— ì¶”ê°€
            if frame_predictions:
                predictions.append(frame_predictions)

        return predictions

    def postprocess(self, detections, num_workers=2):
        """ ğŸ”¥ ë‹¤ì¤‘ ì˜ìƒ ë³‘ë ¬ í›„ì²˜ë¦¬ """
        results = []
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [executor.submit(self.postprocess_single_video, idx, detections[idx])
                       for idx in range(len(detections))]

            for future in futures:
                results.append(future.result())

        return results

    def handle(self, data, context):
        """ ğŸ”¥ ìš”ì²­ ì²˜ë¦¬ (ì „ì²˜ë¦¬ â†’ ì¶”ë¡  â†’ í›„ì²˜ë¦¬) """
        preprocessed_data = self.preprocess(data)
        if not preprocessed_data or not preprocessed_data["videos"]:
            return [{"results": []}]  # âœ… ë¹ˆ ì‘ë‹µë„ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ ë°˜í™˜

        detections = self.inference(preprocessed_data)
        response = self.postprocess(detections)

        # ğŸ”¥ TorchServe í˜¸í™˜ ì‘ë‹µ ë°˜í™˜
        return [{"results": response}]  # âœ… ë°˜ë“œì‹œ ë¦¬ìŠ¤íŠ¸ë¡œ ê°ì‹¸ì„œ ë°˜í™˜
