import torch
import os
import cv2
import numpy as np
from concurrent.futures import ThreadPoolExecutor
from ts.torch_handler.base_handler import BaseHandler
from ultralytics import YOLO  # YOLOv8 ëª¨ë¸ ë¡œë“œ
from torchvision import transforms
from PIL import Image

# âœ… ì„œë²„ ì‹¤í–‰ ì‹œ í•œ ë²ˆë§Œ MiDaS ëª¨ë¸ ë¡œë“œ
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("ğŸ”„ Loading MiDaS model (server initialization)...")
midas = torch.hub.load("intel-isl/MiDaS", "MiDaS_small").to(device)
midas.eval()
print("âœ… MiDaS Model loaded successfully (server-wide)")

# âœ… MiDaS ì…ë ¥ ë³€í™˜ ì„¤ì • (ì„œë²„ ì „ì—­ ì‚¬ìš©)
midas_transform = transforms.Compose([
    transforms.Resize((256, 256)),  # MiDaS Small ëª¨ë¸ í•´ìƒë„
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.5], std=[0.5])
])


class YOLOv8Handler(BaseHandler):
    def initialize(self, context):
        """ ëª¨ë¸ ì´ˆê¸°í™” """
        model_dir = context.system_properties.get("model_dir")

        # âœ… YOLOv8 ëª¨ë¸ ë¡œë“œ
        model_path = os.path.join(model_dir, "yolov8n.pt")
        self.model = YOLO(model_path).to(device)
        self.model.eval()
        print("âœ… YOLOv8 Model loaded successfully")

        # âœ… MiDaSëŠ” ì„œë²„ ì „ì—­ ë³€ìˆ˜ ì‚¬ìš©
        self.midas = midas
        self.midas_transform = midas_transform

    def run_midas(self, frame):
        """ MiDaSë¡œ ê¹Šì´ ë§µ ì¶”ì¶œ """
        try:
            # 1. ì…ë ¥ í”„ë ˆì„ ì •ë³´ ì¶œë ¥
            print(f"Input frame - Shape: {frame.shape}, Type: {frame.dtype}, Range: [{frame.min()}, {frame.max()}]")
            
            # 2. ì…ë ¥ ê²€ì¦
            if len(frame.shape) != 3 or frame.shape[-1] != 3:
                raise ValueError(f"Invalid frame shape: {frame.shape}, expected (H, W, 3)")
            
            # 3. float32 -> uint8 ë³€í™˜ (ë²”ìœ„ ì¡°ì •)
            if frame.dtype == np.float32:
                # [-1,1] ë˜ëŠ” [0,1] ë²”ìœ„ë¥¼ [0,255]ë¡œ ë³€í™˜
                if frame.min() < 0:  # [-1,1] ë²”ìœ„ì¸ ê²½ìš°
                    frame = ((frame + 1.0) * 127.5).astype(np.uint8)
                else:  # [0,1] ë²”ìœ„ì¸ ê²½ìš°
                    frame = (frame * 255).astype(np.uint8)
            
            # 4. uint8 íƒ€ì… í™•ì¸
            if frame.dtype != np.uint8:
                frame = frame.astype(np.uint8)
            
            # 5. BGR -> RGB ë³€í™˜ (uint8 í˜•ì‹ìœ¼ë¡œ)
            img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            print(f"After conversion - Shape: {img_rgb.shape}, Type: {img_rgb.dtype}, Range: [{img_rgb.min()}, {img_rgb.max()}]")
            
            # 6. PIL Image ë³€í™˜
            img_pil = Image.fromarray(img_rgb, mode='RGB')
            
            # 7. MiDaS ë³€í™˜ ë° ì¶”ë¡ 
            img_tensor = self.midas_transform(img_pil).unsqueeze(0).to(device)
            
            with torch.no_grad():
                depth_map = self.midas(img_tensor)
            
            # 8. ê¹Šì´ ë§µ í›„ì²˜ë¦¬
            depth_map = depth_map.squeeze().cpu().numpy()
            depth_map = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)
            
            return depth_map
            
        except Exception as e:
            print(f"MiDaS processing error: {str(e)}")
            print(f"Frame info at error: Shape={frame.shape}, Type={frame.dtype}")
            raise


    def preprocess(self, data, num_workers=2):
        """ ë³‘ë ¬ ì˜ìƒ ì „ì²˜ë¦¬ (ë‹¨ì¼/ë‹¤ì¤‘ ì˜ìƒ ì§€ì›) """
        def process_video(idx, video_bytes):
            video_path = f"/tmp/temp_video_{idx}.mp4"
            with open(video_path, "wb") as f:
                f.write(video_bytes)

            cap = cv2.VideoCapture(video_path)
            frames = []
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret:
                    break
                frame = cv2.resize(frame, (320, 256))
                frame = torch.from_numpy(frame).permute(2, 0, 1).float().to(self.device) / 255.0
                frames.append(frame)
            cap.release()
            return frames

        videos = []
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            futures = [executor.submit(process_video, idx, video_data.get("body", None)) for idx, video_data in enumerate(data)]
            for future in futures:
                videos.append(future.result())

        return {"videos": videos}

    def inference(self, data, batch_size=1):
        """ YOLOv8 + MiDaS ë°°ì¹˜ ì¶”ë¡  """
        results = []
        with torch.no_grad():
            for video_idx, frames in enumerate(data["videos"]):
                video_results = []
                for i in range(0, len(frames), batch_size):
                    batch = torch.stack(frames[i:i+batch_size]) if len(frames[i:i+batch_size]) > 1 else frames[i:i+batch_size][0].unsqueeze(0)
                    
                    # YOLOv8 ê°ì²´ íƒì§€ ì‹¤í–‰
                    batch_results = self.model(batch, verbose=False)
                    
                    # YOLO ê°ì§€ ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³  MiDaS ì‹¤í–‰
                    for j, result in enumerate(batch_results):
                        if result.boxes is not None and len(result.boxes.xyxy) > 0:
                            # torch tensor -> numpy array ë³€í™˜ (ê°’ ë²”ìœ„ ë³´ì¡´)
                            frame = frames[i + j].permute(1, 2, 0).cpu().numpy()
                            print(f"Frame before MiDaS - Shape: {frame.shape}, Type: {frame.dtype}")
                            
                            try:
                                depth_map = self.run_midas(frame)
                                result.depth_map = depth_map
                            except Exception as e:
                                print(f"MiDaS ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                                result.depth_map = None
                        else:
                            result.depth_map = None
                    
                    video_results.extend(batch_results)
                
                results.append(video_results)
                
        return results


    def run_midas(self, frame):
        """ MiDaSë¡œ ê¹Šì´ ë§µ ì¶”ì¶œ """
        try:
            print("ğŸ”„ Starting MiDaS processing...")
            
            # 1. ì…ë ¥ í”„ë ˆì„ ì •ë³´
            print(f"Input frame - Shape: {frame.shape}, Type: {frame.dtype}, Range: [{frame.min()}, {frame.max()}]")
            
            # 2. uint8 ë³€í™˜
            if frame.dtype == np.float32:
                frame = (frame * 255).astype(np.uint8)
            
            # 3. RGB ë³€í™˜
            img_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            print(f"After RGB conversion - Shape: {img_rgb.shape}, Type: {img_rgb.dtype}")
            
            # 4. PIL ë³€í™˜
            img_pil = Image.fromarray(img_rgb, 'RGB')
            
            # 5. MiDaS ì…ë ¥ ì¤€ë¹„
            img_tensor = self.midas_transform(img_pil).unsqueeze(0).to(device)
            print(f"Tensor ready for MiDaS - Shape: {img_tensor.shape}")
            
            # 6. MiDaS ì¶”ë¡ 
            with torch.no_grad():
                depth_map = self.midas(img_tensor)
            
            # 7. í›„ì²˜ë¦¬
            depth_map = depth_map.squeeze().cpu().numpy()
            print(f"Raw depth map - Shape: {depth_map.shape}, Range: [{depth_map.min()}, {depth_map.max()}]")
            
            # 8. ì •ê·œí™”
            depth_map = cv2.normalize(depth_map, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)
            print(f"Normalized depth map - Shape: {depth_map.shape}, Range: [{depth_map.min()}, {depth_map.max()}]")
            
            return depth_map
            
        except Exception as e:
            print(f"âŒ MiDaS error: {str(e)}")
            print(f"Frame info at error: Shape={frame.shape}, Type={frame.dtype}")
            raise

    def postprocess(self, data):
        """ YOLOv8 + MiDaS í›„ì²˜ë¦¬ """
        all_predictions = []
        for video_idx, video_results in enumerate(data):
            video_predictions = []
            for result in video_results:
                boxes = result.boxes.xyxy.cpu().numpy()
                scores = result.boxes.conf.cpu().numpy()
                classes = result.boxes.cls.cpu().numpy()
                depth_map = result.depth_map  # MiDaS ê¹Šì´ ë§µ

                frame_predictions = []
                for box, score, cls in zip(boxes, scores, classes):
                    x1, y1, x2, y2 = map(int, box)
                    class_id = int(cls)

                    # ê¹Šì´ ê³„ì‚° ë””ë²„ê¹…
                    if depth_map is not None:
                        try:
                            # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œê°€ ìœ íš¨í•œì§€ í™•ì¸
                            h, w = depth_map.shape
                            x1 = max(0, min(x1, w-1))
                            x2 = max(0, min(x2, w-1))
                            y1 = max(0, min(y1, h-1))
                            y2 = max(0, min(y2, h-1))
                            
                            # ROI ì¶”ì¶œ
                            roi = depth_map[y1:y2, x1:x2]
                            
                            if roi.size > 0:
                                # ROI í†µê³„ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
                                print(f"ROI shape: {roi.shape}")
                                print(f"ROI range: [{roi.min()}, {roi.max()}]")
                                print(f"ROI mean before round: {np.mean(roi)}")
                                
                                # í‰ê·  ê¹Šì´ ê³„ì‚° ë° ë°˜ì˜¬ë¦¼
                                Z = int(round(np.mean(roi)))
                                print(f"Final depth value: {Z}")
                            else:
                                print("âŒ ROI is empty")
                                Z = 0
                        except Exception as e:
                            print(f"âŒ Depth calculation error: {str(e)}")
                            Z = 0
                    else:
                        print("âŒ Depth map is None")
                        Z = 0

                    frame_predictions.append({
                        "bbox": box.tolist(),
                        "confidence": float(score),
                        "class_id": class_id,
                        "class_name": self.model.names[class_id],
                        "depth": Z
                    })

                video_predictions.append(frame_predictions)
                print(f"ğŸ“Š Frame predictions: {frame_predictions}")  # ì „ì²´ ì˜ˆì¸¡ ê²°ê³¼ ì¶œë ¥

            all_predictions.append(video_predictions)
            print(f"ğŸ“Œ Video {video_idx}: Total Predictions {len(video_predictions)} Frames")
        return all_predictions
